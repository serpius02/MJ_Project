import asyncio
import json
import os
from datetime import datetime, timedelta, timezone

import feedparser
import openai
from dateutil import parser as date_parser
from dotenv import load_dotenv
from supabase import Client, create_client

# ì´ ì½”ë“œëŠ” ì›¹ì‚¬ì´íŠ¸ê°€ ì •í˜•í™”ëœ rss feed ì£¼ì†Œë¥¼ ì œê³µí•˜ëŠ” ê²½ìš°ì— ì‚¬ìš©
# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# --- í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    SUPABASE_URL = os.environ["SUPABASE_URL"]
    SUPABASE_KEY = os.environ["SUPABASE_SERVICE_ROLE_KEY"]
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    openai_client = openai.AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])
except KeyError as e:
    print(f"ì˜¤ë¥˜: í™˜ê²½ ë³€ìˆ˜ {e}ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit()

# --- ì„¤ì • ---
RSS_FEEDS = [
    "https://rss.nytimes.com/services/xml/rss/nyt/Education.xml",
]
SCRIPT_NAME = "rss_parser_script"

# --- ì†ŒìŠ¤ ì´ë¦„ ë§¤í•‘ ---
SOURCE_MAP = {
    "https://rss.nytimes.com/services/xml/rss/nyt/Education.xml": "The New York Times",
    # ë‹¤ë¥¸ í”¼ë“œë¥¼ ì¶”ê°€í•  ë•Œ ì—¬ê¸°ì— ë“±ë¡: "í”¼ë“œì£¼ì†Œ": "í‘œì‹œí•  ì´ë¦„"
}

# --- AI ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ì´ì „ê³¼ ë™ì¼) ---


async def get_embedding(text: str) -> list[float]:
    """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        response = await openai_client.embeddings.create(
            input=text, model="text-embedding-3-small"
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return []


async def get_ai_summary_and_translation(
    title: str, summary: str, source_name: str
) -> dict:
    """AIë¥¼ ì´ìš©í•´ ë‰´ìŠ¤ í˜•ì‹ì˜ ìš”ì•½ê³¼ ë²ˆì—­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    system_prompt = f"""
You are a professional international news editor. Your task is to process a news brief from "{source_name}" and generate a JSON object with two keys: "en" and "ko".

Each key should contain an object with "title" and "summary" keys.

- For the "en" key:
  - "title": The original English title.
  - "summary": A concise, news-style summary beginning with "{source_name} reports that...".

- For the "ko" key:
  - "title": A high-quality Korean translation of the original title.
  - "summary": A news-style summary in Korean beginning with "{source_name}ì— ë”°ë¥´ë©´...".

Ensure the summaries are objective and reportorial in tone.
Your entire output must be a single, valid JSON object.
"""
    content_to_process = f"Title: {title}\n\nSummary: {summary}"
    try:
        response = await openai_client.chat.completions.create(
            model=os.getenv("LLM_MODEL", "gpt-4o-mini"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content_to_process},
            ],
            response_format={"type": "json_object"},
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"ë²ˆì—­/ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
        return {"en": {"title": title, "summary": summary}}


async def process_entry(entry, clean_source_name):
    """RSS í”¼ë“œì˜ ê°œë³„ í•­ëª©(entry)ì„ ë°›ì•„ ëª¨ë“  AI ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    title = entry.get("title", "")
    summary = entry.get("summary", "")
    link = entry.get("link")
    published_str = entry.get("published", "")

    if not all([title, link, published_str]):
        return None

    text_to_embed = f"Title: {title}\nSummary: {summary}"
    embedding_task = get_embedding(text_to_embed)
    translation_task = get_ai_summary_and_translation(title, summary, clean_source_name)
    embedding, translations = await asyncio.gather(embedding_task, translation_task)

    if not embedding:
        return None

    return {
        "title": title,
        "link": link,
        "summary": summary,
        "published_date": date_parser.parse(published_str).isoformat(),
        "source": clean_source_name,
        "embedding": embedding,
        "translations": translations,
    }


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
async def main():
    print("ğŸš€ AI ê¸°ë°˜ RSS íŒŒì„œ ì‹¤í–‰ (DB ìƒíƒœ ê¸°ì–µ ë°©ì‹)...")
    start_time = datetime.now(timezone.utc)
    time_threshold = None

    # 1. DBì—ì„œ ë§ˆì§€ë§‰ ì„±ê³µ ì‹¤í–‰ ì‹œê°„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    try:
        response = (
            supabase.table("script_metadata")
            .select("last_successful_run")
            .eq("script_name", SCRIPT_NAME)
            .single()
            .execute()
        )
        if response.data:
            time_threshold = date_parser.parse(response.data["last_successful_run"])
            print(
                f"ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„: {time_threshold.strftime('%Y-%m-%d %H:%M')} ì´í›„ ë‰´ìŠ¤ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
            )
        else:
            # ê¸°ë¡ì´ ì—†ìœ¼ë©´ ìµœì´ˆ ì‹¤í–‰ìœ¼ë¡œ ê°„ì£¼
            time_threshold = start_time - timedelta(days=3)
            print("ìµœì´ˆ ì‹¤í–‰: ì§€ë‚œ 3ì¼ê°„ì˜ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
    except Exception:
        # í…Œì´ë¸”ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìµœì´ˆ ì‹¤í–‰ìœ¼ë¡œ ê°„ì£¼
        time_threshold = start_time - timedelta(days=3)
        print("ìµœì´ˆ ì‹¤í–‰ ë˜ëŠ” ì¡°íšŒ ì˜¤ë¥˜: ì§€ë‚œ 3ì¼ê°„ì˜ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # 2. ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ í•„í„°ë§í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
    tasks = []
    for feed_url in RSS_FEEDS:
        print(f"\ní”¼ë“œ í™•ì¸ ì¤‘: {feed_url}")
        feed = feedparser.parse(feed_url)
        # â–¼â–¼â–¼ [ìˆ˜ì •] SOURCE_MAPì„ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½ â–¼â–¼â–¼
        clean_source_name = SOURCE_MAP.get(feed_url, feed.feed.get("title", feed_url))

        for entry in feed.entries:
            published_date = date_parser.parse(entry.get("published", ""))
            if published_date > time_threshold:
                # â–¼â–¼â–¼ [ìˆ˜ì •] clean_source_nameì„ ì¸ìë¡œ ì „ë‹¬ â–¼â–¼â–¼
                tasks.append(process_entry(entry, clean_source_name))

    if not tasks:
        print("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    processed_articles = await asyncio.gather(*tasks)
    articles_to_insert = [article for article in processed_articles if article]

    # 3. ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ DBì— ì €ì¥í•˜ê³ , ì„±ê³µí•˜ë©´ ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    try:
        if articles_to_insert:
            supabase.table("news_articles").upsert(
                articles_to_insert, on_conflict="link"
            ).execute()
            print(
                f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! {len(articles_to_insert)}ê°œ í•­ëª©ì„ Supabaseì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."
            )
        else:
            print("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ì´ ëë‚¬ìœ¼ë¯€ë¡œ DBì— ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        supabase.table("script_metadata").upsert(
            {"script_name": SCRIPT_NAME, "last_successful_run": start_time.isoformat()}
        ).execute()
        print(f"ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ê¸°ë¡ ì™„ë£Œ: {start_time.strftime('%Y-%m-%d %H:%M')}")

    except Exception as e:
        print(f"ğŸš¨ Supabase ë°ì´í„° ì €ì¥/ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        print("ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ì„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main())
