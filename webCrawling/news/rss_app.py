import asyncio
import json
import os
import time
from datetime import datetime, timedelta, timezone

import feedparser
import openai
import requests  # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from dateutil import parser as date_parser
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from supabase import Client, create_client
from urllib3.util.retry import Retry

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# --- í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    SUPABASE_URL = os.environ["SUPABASE_URL"]
    SUPABASE_KEY = os.environ["SUPABASE_SERVICE_ROLE_KEY"]
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    openai_client = openai.AsyncOpenAI(
        api_key=os.environ["OPENAI_API_KEY"], timeout=60.0
    )
except KeyError as e:
    print(f"ì˜¤ë¥˜: í™˜ê²½ ë³€ìˆ˜ {e}ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit()

# --- ì„¤ì • ---
RSS_FEEDS = [
    "https://rss.app/feeds/MBYAZYmRL2O5aPji.xml",
    "https://rss.app/feeds/KQdogDhcsq2I7nXM.xml",
    "https://rss.app/feeds/2YBnLkuviM19kHiB.xml",
    "https://rss.app/feeds/Zh5N6av0PKMopfyF.xml",
]
SCRIPT_NAME = "rss_app"

# --- ì†ŒìŠ¤ ì´ë¦„ ë§¤í•‘ ---
SOURCE_MAP = {
    "https://rss.app/feeds/MBYAZYmRL2O5aPji.xml": "The New York Times",
    "https://rss.app/feeds/KQdogDhcsq2I7nXM.xml": "Inside Higher Ed",
    "https://rss.app/feeds/2YBnLkuviM19kHiB.xml": "US News & World Report",
    "https://rss.app/feeds/Zh5N6av0PKMopfyF.xml": "The Chronicle of Higher Education",
}


# --- ë„¤íŠ¸ì›Œí¬ ìš”ì²­ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „) ---
async def fetch_feed_content_async(url: str, max_retries: int = 3):
    """ê°œì„ ëœ RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜"""
    print(f"  -> ë„¤íŠ¸ì›Œí‚¹ ì‹œì‘: {url}")
    loop = asyncio.get_running_loop()

    def blocking_get_with_retry():
        # ì„¸ì…˜ ìƒì„± ë° ì¬ì‹œë„ ì „ëµ ì„¤ì •
        session = requests.Session()

        # ì¬ì‹œë„ ì „ëµ ì„¤ì •
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # ë” í˜„ì‹¤ì ì¸ í—¤ë” ì„¤ì •
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/rss+xml, application/xml, text/xml, */*",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

        for attempt in range(max_retries):
            try:
                print(f"    ì‹œë„ {attempt + 1}/{max_retries}")

                # íƒ€ì„ì•„ì›ƒì„ ë” ê¸¸ê²Œ ì„¤ì • (ì—°ê²°: 10ì´ˆ, ì½ê¸°: 30ì´ˆ)
                response = session.get(
                    url,
                    headers=headers,
                    timeout=(10, 30),
                    allow_redirects=True,
                    verify=True,
                )
                response.raise_for_status()

                print(
                    f"    âœ… ì„±ê³µ: ìƒíƒœì½”ë“œ {response.status_code}, í¬ê¸°: {len(response.content)} bytes"
                )
                return response.content

            except requests.exceptions.Timeout as e:
                print(f"    â° íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # 2ì´ˆ, 4ì´ˆ, 6ì´ˆ ëŒ€ê¸°
                    print(f"    ğŸ’¤ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)

            except requests.exceptions.ConnectionError as e:
                print(f"    ğŸ”Œ ì—°ê²° ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep((attempt + 1) * 3)

            except requests.exceptions.HTTPError as e:
                print(f"    ğŸ“¡ HTTP ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if e.response.status_code in [429, 503]:  # ì¼ì‹œì  ì˜¤ë¥˜
                    if attempt < max_retries - 1:
                        time.sleep((attempt + 1) * 5)
                else:
                    break  # ì˜êµ¬ì  ì˜¤ë¥˜ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ

            except requests.exceptions.RequestException as e:
                print(f"    âŒ ê¸°íƒ€ ìš”ì²­ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep((attempt + 1) * 2)

        print(f"    ğŸ’¥ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨: {url}")
        return None

    # ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
    content = await loop.run_in_executor(None, blocking_get_with_retry)
    print(f"  <- ë„¤íŠ¸ì›Œí‚¹ ì™„ë£Œ: {url}")
    return content


# --- AI ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ì´ì „ê³¼ ë™ì¼) ---
# ... get_embedding, get_ai_summary_and_translation, process_entry í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ...
async def get_embedding(text: str) -> list[float]:
    """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        response = await openai_client.embeddings.create(
            input=text, model="text-embedding-3-small"
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return []


async def get_ai_summary_and_translation(
    title: str, summary: str, source_name: str
) -> dict:
    """AIë¥¼ ì´ìš©í•´ ë‰´ìŠ¤ í˜•ì‹ì˜ ìš”ì•½ê³¼ ë²ˆì—­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    system_prompt = f"""
You are a professional international news editor. Your task is to process a news brief from "{source_name}" and generate a JSON object with two keys: "en" and "ko".
Each key should contain an object with "title" and "summary" keys.
- For the "en" key:
  - "title": The original English title.
  - "summary": A concise, news-style summary beginning with "{source_name} reports that...".
- For the "ko" key:
  - "title": A high-quality Korean translation of the original title.
  - "summary": A news-style summary in Korean beginning with "{source_name}ì— ë”°ë¥´ë©´...".
Ensure the summaries are objective and reportorial in tone. Your entire output must be a single, valid JSON object.
"""
    content_to_process = f"Title: {title}\n\nSummary: {summary}"
    try:
        response = await openai_client.chat.completions.create(
            model=os.getenv("LLM_MODEL", "gpt-4o-mini"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content_to_process},
            ],
            response_format={"type": "json_object"},
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"ë²ˆì—­/ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
        return {"en": {"title": title, "summary": summary}}


async def process_entry(entry, clean_source_name):
    """RSS í”¼ë“œì˜ ê°œë³„ í•­ëª©(entry)ì„ ë°›ì•„ ëª¨ë“  AI ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    title = entry.get("title", "")
    summary = entry.get("summary", "")
    link = entry.get("link")
    published_str = entry.get("published", "")

    if not all([title, link, published_str]):
        return None

    text_to_embed = f"Title: {title}\nSummary: {summary}"
    embedding_task = get_embedding(text_to_embed)
    translation_task = get_ai_summary_and_translation(title, summary, clean_source_name)
    embedding, translations = await asyncio.gather(embedding_task, translation_task)

    if not embedding:
        return None

    return {
        "title": title,
        "link": link,
        "summary": summary,
        "published_date": date_parser.parse(published_str).isoformat(),
        "source": clean_source_name,
        "embedding": embedding,
        "translations": translations,
    }


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
async def main():
    print("ğŸš€ AI ê¸°ë°˜ RSS íŒŒì„œ ì‹¤í–‰ (DB ìƒíƒœ ê¸°ì–µ ë°©ì‹)...")
    start_time = datetime.now(timezone.utc)
    # ... (DBì—ì„œ ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ê°€ì ¸ì˜¤ëŠ” ë¡œì§ì€ ê·¸ëŒ€ë¡œ) ...
    time_threshold = None
    try:
        response = (
            supabase.table("script_metadata")
            .select("last_successful_run")
            .eq("script_name", SCRIPT_NAME)
            .single()
            .execute()
        )
        if response.data:
            time_threshold = date_parser.parse(response.data["last_successful_run"])
            print(
                f"ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„: {time_threshold.strftime('%Y-%m-%d %H:%M')} ì´í›„ ë‰´ìŠ¤ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
            )
        else:
            time_threshold = start_time - timedelta(days=5)
            print("ìµœì´ˆ ì‹¤í–‰: ì§€ë‚œ 5ì¼ê°„ì˜ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
    except Exception:
        time_threshold = start_time - timedelta(days=5)
        print("ìµœì´ˆ ì‹¤í–‰ ë˜ëŠ” ì¡°íšŒ ì˜¤ë¥˜: ì§€ë‚œ 5ì¼ê°„ì˜ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # 2. ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì•„ ì²˜ë¦¬í•  ì‘ì—… ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤.
    tasks = []
    print("\n[1ë‹¨ê³„] ê° í”¼ë“œì—ì„œ ì²˜ë¦¬í•  ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤...")
    for feed_url in RSS_FEEDS:
        print(f"\n  - í”¼ë“œ í™•ì¸: {feed_url}")

        # ìˆ˜ì •ëœ ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ í”¼ë“œ ë‚´ìš©ì„ ê°€ì ¸ì˜´
        feed_content = await fetch_feed_content_async(feed_url)

        if not feed_content:
            continue  # í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ì‹œ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°

        feed = feedparser.parse(feed_content)
        clean_source_name = SOURCE_MAP.get(feed_url, feed.feed.get("title", feed_url))

        for entry in feed.entries:
            if "published" not in entry:
                continue

            published_date = date_parser.parse(entry.get("published"))

            if published_date.tzinfo is None:
                published_date = published_date.replace(tzinfo=timezone.utc)

            if published_date > time_threshold:
                tasks.append(process_entry(entry, clean_source_name))

    # ... (ì´í•˜ ë°°ì¹˜ ì²˜ë¦¬ ë° DB ì €ì¥ ë¡œì§ì€ ì´ì „ ë‹µë³€ê³¼ ë™ì¼) ...
    if not tasks:
        print("\nì²˜ë¦¬í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    articles_to_insert = []
    total_tasks = len(tasks)
    batch_size = 5

    print(
        f"\n[2ë‹¨ê³„] ì´ {total_tasks}ê°œì˜ ê¸°ì‚¬ë¥¼ {batch_size}ê°œì”© ë¬¶ì–´ AI ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
    )

    for i in range(0, total_tasks, batch_size):
        batch_tasks = tasks[i : i + batch_size]
        print(
            f"\n  -> ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘ ({i+1}~{min(i+batch_size, total_tasks)}ë²ˆ ì‘ì—…)..."
        )

        for future in asyncio.as_completed(batch_tasks):
            try:
                article = await future
                if article:
                    articles_to_insert.append(article)
                    print(f"    â””> AI ì²˜ë¦¬ ì™„ë£Œ: {article['title'][:50]}...")
            except Exception as e:
                print(f"    â””> AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í•­ëª© ê±´ë„ˆëœ€): {e}")

    print(
        f"\n[3ë‹¨ê³„] AI ì²˜ë¦¬ ì™„ë£Œ. {len(articles_to_insert)}ê°œì˜ ê¸°ì‚¬ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤..."
    )
    try:
        if articles_to_insert:
            supabase.table("news_articles").upsert(
                articles_to_insert, on_conflict="link"
            ).execute()
            print(
                f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! {len(articles_to_insert)}ê°œ í•­ëª©ì„ Supabaseì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."
            )
        else:
            print("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ëª¨ë“  í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        supabase.table("script_metadata").upsert(
            {"script_name": SCRIPT_NAME, "last_successful_run": start_time.isoformat()}
        ).execute()
        print(f"ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ ê¸°ë¡ ì™„ë£Œ: {start_time.strftime('%Y-%m-%d %H:%M')}")

    except Exception as e:
        print(f"ğŸš¨ Supabase ë°ì´í„° ì €ì¥/ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        print("ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ì„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main())
