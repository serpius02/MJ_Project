## ì£¼ì–´ì§„ capplica data ì…‹ì„ í†µì§¸ë¡œ ì–¸ì–´ ëª¨ë¸ì— ë„£ì–´ì„œ bilingual structured outputs ë½‘ì•„ë‚´ëŠ” ì½”ë“œ

import json
import os
from typing import List, Literal, Optional

import dspy
import mlflow
import pandas as pd
import pydantic
from dotenv import load_dotenv
from tqdm import tqdm

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# MLflow ì„¤ì •
mlflow.set_tracking_uri("file:./mlruns")
mlflow.set_experiment("batch_processing_bilingual")
mlflow.dspy.autolog()

# --- 1. Pydantic ëª¨ë¸ ì •ì˜ (single_post_final.pyì—ì„œ ë³µì‚¬) ---


class LocalizedText(pydantic.BaseModel):
    """Represents a piece of text in both English and Korean."""

    english: Optional[str] = None
    korean: Optional[str] = None


class Demographics(pydantic.BaseModel):
    gender: Optional[str] = None
    race_ethnicity: Optional[LocalizedText] = None
    residence: Optional[LocalizedText] = None
    income_bracket: Optional[str] = None
    type_of_school: Optional[LocalizedText] = None
    hooks: Optional[List[LocalizedText]] = None
    intended_majors: Optional[LocalizedText] = None


class GPA(pydantic.BaseModel):
    unweighted: Optional[float] = None
    weighted: Optional[float] = None


class CourseCount(pydantic.BaseModel):
    honors: Optional[int] = None
    ap: Optional[int] = None
    ib: Optional[int] = None
    dual_enrollment: Optional[int] = None


class Academics(pydantic.BaseModel):
    gpa: Optional[GPA] = None
    course_count: Optional[CourseCount] = None
    senior_year_course_load: Optional[List[LocalizedText]] = None


class SAT(pydantic.BaseModel):
    total: Optional[int] = None
    reading_writing: Optional[int] = None
    math: Optional[int] = None


class StandardizedTesting(pydantic.BaseModel):
    sat_i: Optional[SAT] = None
    act: Optional[int] = None


class LOR(pydantic.BaseModel):
    recommender: Optional[LocalizedText] = None
    estimated_rating: Optional[float] = None


class Essay(pydantic.BaseModel):
    common_app: Optional[LocalizedText] = None
    supplementals: Optional[LocalizedText] = None


class Decision(pydantic.BaseModel):
    university: LocalizedText
    decision_plan: Literal["EA", "ED", "RD", "RA", "N/A"]


class Decisions(pydantic.BaseModel):
    acceptances: Optional[List[Decision]] = None
    waitlists: Optional[List[Decision]] = None
    rejections: Optional[List[Decision]] = None


# ëª¨ë“  ê²ƒì„ í¬í•¨í•˜ëŠ” ìµœìƒìœ„ Pydantic ëª¨ë¸
class StudentProfile(pydantic.BaseModel):
    demographics: Optional[Demographics] = None
    academics: Optional[Academics] = None
    standardized_testing: Optional[StandardizedTesting] = None
    extracurriculars: Optional[List[LocalizedText]] = None
    awards_honors: Optional[List[LocalizedText]] = None
    letters_of_recommendation: Optional[List[LOR]] = None

    essays: Optional[Essay] = None
    decisions: Optional[Decisions] = None
    additional_information: Optional[LocalizedText] = None

    class Config:
        json_schema_extra = {
            "description": "A structured representation of a student's college application profile."
        }


# --- 2. DSPy Signature ì •ì˜ ---


class ExtractProfile(dspy.Signature):
    """
    You are an expert in parsing and structuring student college application data.
    Extract detailed information from the student's post into the given JSON schema.

    Key Instructions:
    1.  **LocalizedText Fields - Translation Rules**:
        - **English-only fields** (set korean: null): race_ethnicity, residence, type_of_school, university
        - **Translate to Korean**: hooks, intended_majors, senior_year_course_load, recommender, common_app, supplementals, extracurriculars, awards_honors, additional_information
        - For English-only fields: {"english": "extracted_text", "korean": null}
        - For translated fields: {"english": "extracted_text", "korean": "korean_translation"}

    2.  **Non-LocalizedText Fields**: gender, income_bracket remain as simple strings/values (not LocalizedText objects)

    3.  **Decision Plan**: For the 'decision_plan' field, classify based on when the final decision was made, not the initial application type. If a student was deferred or waitlisted and then later accepted, the plan should reflect the timing of that final acceptance (e.g., 'RD'). If there's no mention of the application type, use 'N/A'.

    4.  **Completeness**: Fill in all possible fields. If information for a field is not available, leave it as null.
    """

    post_text: str = dspy.InputField(desc="The full text of the student's post.")
    structured_profile: StudentProfile = dspy.OutputField(
        desc="The structured student profile in bilingual JSON format."
    )


# --- 3. DSPy ì„¤ì • ---


def setup_dspy():
    """DSPy ì„¤ì •"""
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

    if not GEMINI_API_KEY:
        raise ValueError(
            "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            ".env íŒŒì¼ì— GEMINI_API_KEY=your_api_key_here ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
        )

    dspy.configure(
        lm=dspy.LM(
            "gemini/gemini-2.5-flash",
            temperature=0,
            max_tokens=8000,
            api_key=GEMINI_API_KEY,
        )
    )


# --- 4. ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ ---


def load_excel_data(excel_path):
    """Excel íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“ Excel íŒŒì¼ ë¡œë”©: {excel_path}")

    df = pd.read_excel(excel_path)
    print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")

    if "json_friendly_post" not in df.columns:
        raise ValueError("'json_friendly_post' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ë¹ˆ ê°’ì´ ì•„ë‹Œ í–‰ë§Œ í•„í„°ë§
    df = df.dropna(subset=["json_friendly_post"])
    print(f"ğŸ“Š ë¹ˆ ê°’ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {df.shape}")

    return df


def batch_process_posts(
    df, output_path="capplica_application_bilingual_final.json", batch_size=50
):
    """ë°°ì¹˜ë¡œ ê²Œì‹œë¬¼ ì²˜ë¦¬ (Bilingual)"""
    print(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ - ì´ {len(df)}ê°œ ê²Œì‹œë¬¼ (Bilingual)")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}")

    # DSPy ëª¨ë“ˆ ìƒì„±
    extractor = dspy.Predict(ExtractProfile)

    results = []
    errors = []

    # MLflow ì‹¤í–‰
    with mlflow.start_run(run_name="batch_processing_bilingual_full"):
        mlflow.log_param("total_posts", len(df))
        mlflow.log_param("batch_size", batch_size)
        mlflow.log_param("model", "gemini-2.5-flash")
        mlflow.log_param("processing_type", "bilingual")

        # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ tqdm ì‚¬ìš©
        for idx, row in tqdm(
            df.iterrows(), total=len(df), desc="Processing posts (Bilingual)"
        ):
            try:
                post_text = row["json_friendly_post"]

                # DSPyë¡œ êµ¬ì¡°í™”ëœ í”„ë¡œí•„ ì¶”ì¶œ
                result = extractor(post_text=post_text)

                # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
                structured_data = {
                    "post_text": post_text,
                    "structured_profile": result.structured_profile.model_dump(),
                }

                results.append(structured_data)

                # ë°°ì¹˜ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
                if len(results) % batch_size == 0:
                    print(f"âœ… {len(results)}ê°œ ì²˜ë¦¬ ì™„ë£Œ (Bilingual)")

            except Exception as e:
                error_info = {
                    "index": idx,
                    "error": str(e),
                    "post_preview": (
                        str(row["json_friendly_post"])[:100] + "..."
                        if pd.notna(row["json_friendly_post"])
                        else "N/A"
                    ),
                }
                errors.append(error_info)
                print(f"âŒ ì¸ë±ìŠ¤ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

                # ì—ëŸ¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¤‘ë‹¨
                if len(errors) > len(df) * 0.1:  # 10% ì´ìƒ ì—ëŸ¬
                    print(f"âš ï¸ ì—ëŸ¬ìœ¨ì´ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ë‹¨.")
                    break

        # ìµœì¢… ê²°ê³¼ ì €ì¥
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # ì—ëŸ¬ ë¡œê·¸ ì €ì¥ (ìˆë‹¤ë©´)
        if errors:
            error_path = output_path.replace(".json", "_errors.json")
            with open(error_path, "w", encoding="utf-8") as f:
                json.dump(errors, f, indent=2, ensure_ascii=False)
            print(f"âš ï¸ ì—ëŸ¬ ë¡œê·¸ ì €ì¥: {error_path}")

        # í†µê³„ ë¡œê¹…
        mlflow.log_metric("successful_extractions", len(results))
        mlflow.log_metric("failed_extractions", len(errors))
        mlflow.log_metric("success_rate", len(results) / len(df) * 100)

        print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ! (Bilingual)")
        print(f"ğŸ“Š ì„±ê³µ: {len(results)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {len(errors)}ê°œ")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {len(results) / len(df) * 100:.1f}%")

        return results, errors


# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---


def main():
    """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
    print("ğŸ¯ Bilingual ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘!")

    # ê²½ë¡œ ì„¤ì •
    excel_path = "../data_capplica_application/capplica_application_converted.xlsx"
    output_path = (
        "../data_capplica_application/capplica_application_bilingual_final.json"
    )

    try:
        # DSPy ì„¤ì •
        setup_dspy()
        print("âœ… DSPy ì„¤ì • ì™„ë£Œ")

        # ë°ì´í„° ë¡œë“œ
        df = load_excel_data(excel_path)

        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì‘ì€ ìƒ˜í”Œë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸)
        if len(df) > 100:
            print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²˜ìŒ 5ê°œ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ (Bilingual)")
            test_df = df.head(5)
            test_results, test_errors = batch_process_posts(
                test_df, output_path.replace(".json", "_test.json"), batch_size=3
            )

            if len(test_errors) / len(test_df) > 0.3:  # 30% ì´ìƒ ì—ëŸ¬
                print("âš ï¸ í…ŒìŠ¤íŠ¸ì—ì„œ ì—ëŸ¬ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ì „ì²´ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return

            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            if test_results:
                print("\nğŸ“‹ ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
                first_result = test_results[0]["structured_profile"]

                # Demographics ë¯¸ë¦¬ë³´ê¸°
                if first_result.get("demographics"):
                    demo = first_result["demographics"]
                    print("  Demographics:")
                    for key, value in demo.items():
                        if isinstance(value, dict) and "english" in value:
                            print(
                                f"    {key}: '{value['english']}' -> '{value['korean']}'"
                            )
                        elif value is not None:
                            print(f"    {key}: {value}")

                # Extracurriculars ë¯¸ë¦¬ë³´ê¸°
                if (
                    first_result.get("extracurriculars")
                    and len(first_result["extracurriculars"]) > 0
                ):
                    print("  First extracurricular:")
                    first_ec = first_result["extracurriculars"][0]
                    print(f"    '{first_ec['english']}' -> '{first_ec['korean']}'")

            # ì‚¬ìš©ì í™•ì¸
            user_input = input("\ní…ŒìŠ¤íŠ¸ ì„±ê³µ! ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if user_input.lower() != "y":
                print("ì²˜ë¦¬ë¥¼ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return

        # ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬
        results, errors = batch_process_posts(df, output_path, batch_size=30)

        print(f"ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! (Bilingual)")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_path}")

        # ìµœì¢… ë¹„ìš© ì •ë³´
        lm = dspy.settings.lm
        if hasattr(lm, "history"):
            total_cost = sum(
                [x.get("cost", 0) for x in lm.history if x.get("cost") is not None]
            )
            print(f"ğŸ’° ì´ ì˜ˆìƒ ë¹„ìš©: ${total_cost:.4f} USD")

    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
